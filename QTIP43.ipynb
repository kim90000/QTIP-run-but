{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56ba32bc71794d05b6a101a534cad0c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a756b8d2a8634108831377c2515822d3",
              "IPY_MODEL_fea8a2dd073c4ab686bba3cfebfd6890",
              "IPY_MODEL_2719d507d2a94dc996080a944d9e2574"
            ],
            "layout": "IPY_MODEL_ba383158e7444c8db7f177248c55a036"
          }
        },
        "a756b8d2a8634108831377c2515822d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2487c7185cf94f79b2c249e36c87d72d",
            "placeholder": "​",
            "style": "IPY_MODEL_822aa7c1540d48cdb13216ea72c42997",
            "value": "config.json: 100%"
          }
        },
        "fea8a2dd073c4ab686bba3cfebfd6890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_700c3de167914c1f979b0f2c9ffa3ab8",
            "max": 1117,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f8a65c9d3f049888a8db3d6fbecd24c",
            "value": 1117
          }
        },
        "2719d507d2a94dc996080a944d9e2574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47aa155913b944bebae5c2c56f34ac68",
            "placeholder": "​",
            "style": "IPY_MODEL_74964cfe08e04c6eb1fc1ecc2a1afb45",
            "value": " 1.12k/1.12k [00:00&lt;00:00, 79.5kB/s]"
          }
        },
        "ba383158e7444c8db7f177248c55a036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2487c7185cf94f79b2c249e36c87d72d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822aa7c1540d48cdb13216ea72c42997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "700c3de167914c1f979b0f2c9ffa3ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8a65c9d3f049888a8db3d6fbecd24c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47aa155913b944bebae5c2c56f34ac68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74964cfe08e04c6eb1fc1ecc2a1afb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce7069105e1b4048b39b2d6abae3002e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62a4eee1fdd4467abc5c07c6f0e9b345",
              "IPY_MODEL_a33ce85feca442c9a56dddd15536018a",
              "IPY_MODEL_38ba2838a7b147cba999491f31b366fc"
            ],
            "layout": "IPY_MODEL_4448cbe40b71410fbd008a7fdb15429a"
          }
        },
        "62a4eee1fdd4467abc5c07c6f0e9b345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_623747c6b40240e09f1923f88ac5f506",
            "placeholder": "​",
            "style": "IPY_MODEL_5b743d32f79646ca8aea03f7392ab7ec",
            "value": "model.safetensors:  90%"
          }
        },
        "a33ce85feca442c9a56dddd15536018a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a71b4ec730e41cc894ea081d8e506e1",
            "max": 3852517272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6ddee29c2e54dfb91bb7b139c112b1f",
            "value": 3449815040
          }
        },
        "38ba2838a7b147cba999491f31b366fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a67dd6a9a4bb4e84a29849cf7a6c2d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_7ef0f6fe588d4998ac4a2f9736337fad",
            "value": " 3.45G/3.85G [02:25&lt;00:16, 23.9MB/s]"
          }
        },
        "4448cbe40b71410fbd008a7fdb15429a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623747c6b40240e09f1923f88ac5f506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b743d32f79646ca8aea03f7392ab7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a71b4ec730e41cc894ea081d8e506e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6ddee29c2e54dfb91bb7b139c112b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a67dd6a9a4bb4e84a29849cf7a6c2d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ef0f6fe588d4998ac4a2f9736337fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb23e5420f1445dc81454dc6802bf464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7231090105c44f5bb625c00c4abd7e64",
              "IPY_MODEL_029c0073b9c345c28fff5615bd3a83bb",
              "IPY_MODEL_8d756722d98046dba33d7ee6de4c5ff3"
            ],
            "layout": "IPY_MODEL_8870052f96214f8c8ab1e15d06a776f3"
          }
        },
        "7231090105c44f5bb625c00c4abd7e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bff70bd8f29b435ea8052318220b00bb",
            "placeholder": "​",
            "style": "IPY_MODEL_4d4752393e8c45da859913d977817505",
            "value": "model.safetensors: 100%"
          }
        },
        "029c0073b9c345c28fff5615bd3a83bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a77247af9314a2f9785354800ab87cc",
            "max": 3852517272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_615a74708512447dacce8ba002a67588",
            "value": 3852517272
          }
        },
        "8d756722d98046dba33d7ee6de4c5ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32806b92aef84b86bb805d503c28d0e8",
            "placeholder": "​",
            "style": "IPY_MODEL_ec567a599780408186a54eeaf79219a1",
            "value": " 3.85G/3.85G [00:16&lt;00:00, 23.7MB/s]"
          }
        },
        "8870052f96214f8c8ab1e15d06a776f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bff70bd8f29b435ea8052318220b00bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d4752393e8c45da859913d977817505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a77247af9314a2f9785354800ab87cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615a74708512447dacce8ba002a67588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32806b92aef84b86bb805d503c28d0e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec567a599780408186a54eeaf79219a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_rFKebPOIQw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLnyKr7EOWpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token ْْْْْْْْXXXXXXXXXXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ht8cDETO3_0",
        "outputId": "1ebda212-4ac0-4f42-e101-629790a72fcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIaV2FXiPuwt",
        "outputId": "2e6cffc3-f316-4752-bcd4-666b0d45e1e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install huggingface_hub[cli]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gMhbEBRFRZVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!huggingface-cli login --token YOUR_TOKEN"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "y8pKVpw0RfLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"ما هي أهمية الذكاء الاصطناعي في المستقبل؟\"\n",
        "\n",
        "# تنفيذ الاستدلال وتوليد النص\n",
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=200,  # الحد الأقصى لطول النص المُولد\n",
        "    temperature=0.7,  # التحكم في العشوائية\n",
        "    top_k=50,  # اختيار أفضل 50 خيارًا للتوليد\n",
        "    top_p=0.9,  # فلترة الخيارات باستخدام النسبة التراكمية\n",
        "    repetition_penalty=1.2,  # تقليل تكرار الكلمات\n",
        ")\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "56ba32bc71794d05b6a101a534cad0c8",
            "a756b8d2a8634108831377c2515822d3",
            "fea8a2dd073c4ab686bba3cfebfd6890",
            "2719d507d2a94dc996080a944d9e2574",
            "ba383158e7444c8db7f177248c55a036",
            "2487c7185cf94f79b2c249e36c87d72d",
            "822aa7c1540d48cdb13216ea72c42997",
            "700c3de167914c1f979b0f2c9ffa3ab8",
            "6f8a65c9d3f049888a8db3d6fbecd24c",
            "47aa155913b944bebae5c2c56f34ac68",
            "74964cfe08e04c6eb1fc1ecc2a1afb45",
            "ce7069105e1b4048b39b2d6abae3002e",
            "62a4eee1fdd4467abc5c07c6f0e9b345",
            "a33ce85feca442c9a56dddd15536018a",
            "38ba2838a7b147cba999491f31b366fc",
            "4448cbe40b71410fbd008a7fdb15429a",
            "623747c6b40240e09f1923f88ac5f506",
            "5b743d32f79646ca8aea03f7392ab7ec",
            "6a71b4ec730e41cc894ea081d8e506e1",
            "b6ddee29c2e54dfb91bb7b139c112b1f",
            "a67dd6a9a4bb4e84a29849cf7a6c2d0f",
            "7ef0f6fe588d4998ac4a2f9736337fad",
            "bb23e5420f1445dc81454dc6802bf464",
            "7231090105c44f5bb625c00c4abd7e64",
            "029c0073b9c345c28fff5615bd3a83bb",
            "8d756722d98046dba33d7ee6de4c5ff3",
            "8870052f96214f8c8ab1e15d06a776f3",
            "bff70bd8f29b435ea8052318220b00bb",
            "4d4752393e8c45da859913d977817505",
            "5a77247af9314a2f9785354800ab87cc",
            "615a74708512447dacce8ba002a67588",
            "32806b92aef84b86bb805d503c28d0e8",
            "ec567a599780408186a54eeaf79219a1"
          ]
        },
        "id": "zq2nOTsuOWsr",
        "outputId": "ff633cfb-3cb8-4029-c8fd-42445e89916a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ba32bc71794d05b6a101a534cad0c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce7069105e1b4048b39b2d6abae3002e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:  90%|########9 | 3.45G/3.85G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb23e5420f1445dc81454dc6802bf464"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "vCu9HbfdkNUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "Write a love poem.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot who always responds in polite manner\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    max_new_tokens=55,\n",
        ")\n",
        "text=outputs[0][\"generated_text\"][-1]\n",
        "print(text['content'])\n"
      ],
      "metadata": {
        "id": "kofAEGbyOgcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "Write a love poem.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot who always responds in polite manner\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    prompt,\n",
        "    max_new_tokens=55,\n",
        ")\n",
        "text=outputs[0][\"generated_text\"][-1]\n",
        "print(text['content'])"
      ],
      "metadata": {
        "id": "n0zw-DW-lObV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a"
      ],
      "metadata": {
        "id": "k9aVQBQnms6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fahd"
      ],
      "metadata": {
        "id": "sX4Yv0C2nz5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "prompt=\"\"\"\n",
        "Write a love poem with exactly 50 words.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot who always responds in polite manner\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "text=outputs[0][\"generated_text\"][-1]\n",
        "print(text['content'])\n"
      ],
      "metadata": {
        "id": "B-zkaQHxlGfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "# تحميل الـ pipeline\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# النص المُدخل\n",
        "prompt = \"Write a love poem with exactly 50 words.\"\n",
        "\n",
        "# استدعاء النموذج لتوليد النص\n",
        "outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,  # لجعل الاستجابة أكثر تنوعًا\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(outputs[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "tJD9-GMzny52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "# تحديد معرف النموذج\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "# إنشاء خط الأنابيب (Pipeline) لمعالجة النصوص\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",  # يستخدم تلقائيًا الـ GPU إذا كان متاحًا\n",
        ")\n",
        "\n",
        "# إدخال نص كمثال\n",
        "prompt = \"what is ai?\"\n",
        "\n",
        "# تنفيذ الاستدلال وتوليد النص\n",
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=200,  # الحد الأقصى لطول النص المُولد\n",
        "    temperature=0.7,  # التحكم في العشوائية\n",
        "    top_k=50,  # اختيار أفضل 50 خيارًا للتوليد\n",
        "    top_p=0.9,  # فلترة الخيارات باستخدام النسبة التراكمية\n",
        "    repetition_penalty=1.2,  # تقليل تكرار الكلمات\n",
        ")\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "4bI6VqYQZXAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what is ai?\"\n",
        "\n",
        "# تنفيذ الاستدلال وتوليد النص\n",
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=50,  # الحد الأقصى لطول النص المُولد\n",
        "    temperature=0.7,  # التحكم في العشوائية\n",
        "    top_k=50,  # اختيار أفضل 50 خيارًا للتوليد\n",
        "    top_p=0.9,  # فلترة الخيارات باستخدام النسبة التراكمية\n",
        "    repetition_penalty=1.2,  # تقليل تكرار الكلمات\n",
        ")\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "aoD_dDVyZgg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=10,\n",
        "    truncation=True,  # إضافة هذا الخيار لتجنب التحذير\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        ")\n"
      ],
      "metadata": {
        "id": "L54dPKaDaUBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHuwsJCMbyNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ywJZbtEkbyLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",  # يستخدم تلقائيًا الـ GPU إذا كان متاحًا\n",
        ")\n",
        "prompt = \"what is ai?\"\n",
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=10,\n",
        "    truncation=True,  # إضافة هذا الخيار لتجنب التحذير\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        ")\n",
        "print(output[0][\"generated_text\"])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hfq0TkqZbyIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "text = \"What is AI?\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "iZIuY2w5dKSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "prompt = \"What is python?\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "output = model.generate(input_ids, max_length=50, do_sample=True, temperature=0.7, top_k=50, top_p=0.9)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "6WbtTyLbggaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is python?\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, do_sample=True, temperature=0.7, top_k=50, top_p=0.9)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "EN4xT0rigkuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TBW1-TysrbM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Qm3JfOdrbJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ULnhcE1YrbGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uxf_iaMarbBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        " model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        " model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        " text = \"What is AI?\"\n",
        " inputs = tokenizer(text, return_tensors=\"pt\")\n",
        " outputs = model.generate(**inputs, max_length=50)\n",
        " print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "dif3JVa7ra9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        " from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        " model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        " model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        " text = \"What is AI?\"\n",
        " inputs = tokenizer(text, return_tensors=\"pt\")\n",
        " outputs = model.generate(**inputs, max_length=50)\n",
        " print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JmKGFQJr2qk",
        "outputId": "c8e57f24-4fff-458b-b489-e2d475c7c41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f"
      ],
      "metadata": {
        "id": "P2Ni2Bt2r2o-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"1 + 1 = ?\"\n",
        "\n",
        "\n",
        "max_new_tokens\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=5, do_sample=True)"
      ],
      "metadata": {
        "id": "a2-her0er-n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "text = \"What is AI?\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50, do_sample=True)  # Added do_sample=True\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MrEV4Zs0tuPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text = \"1 + 1 = ?\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=9, do_sample=True\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "f2mmgl2Swtfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sont"
      ],
      "metadata": {
        "id": "Lw5eCh05xJ5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text = \"1 + 1 = ?\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=20,  # Increased max_length for a more complete response\n",
        "    do_sample=False  # Set to False for deterministic output\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "CFoQN_LhxJPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model with proper configurations\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "# Load tokenizer with chat template if available\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Format input using proper prompt template for instruction models\n",
        "# Many LLaMA models expect a specific format\n",
        "prompt = \"\"\"<|begin_of_text|><|user|>\n",
        "What is 1 + 1?\n",
        "<|end_of_turn|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "# Generate with more careful parameters\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.1,  # Lower temperature for more deterministic output\n",
        "    do_sample=True,   # Still allow some sampling with low temperature\n",
        "    top_p=0.95,       # Nucleus sampling\n",
        "    repetition_penalty=1.2,  # Penalize repetition\n",
        "    pad_token_id=tokenizer.eos_token_id  # Use EOS as PAD\n",
        ")\n",
        "\n",
        "# Decode only the new tokens, not the input prompt\n",
        "answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "print(f\"Input prompt: {prompt}\")\n",
        "print(f\"Generated answer: {answer}\")"
      ],
      "metadata": {
        "id": "o0t9baGO0VHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Try a different, more standard model\n",
        "model_id = \"meta-llama/Llama-3-8b-instruct\"  # Use the official Meta model if you have access\n",
        "# OR use a different, reliable model:\n",
        "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Load with minimal modifications\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16  # More stable than float16\n",
        ")\n",
        "\n",
        "# Use the model's built-in chat template if available\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is 1 + 1?\"}\n",
        "]\n",
        "\n",
        "# Format with chat template\n",
        "if hasattr(tokenizer, 'apply_chat_template'):\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "else:\n",
        "    # Fallback for older models without chat template\n",
        "    input_text = f\"<s>[INST] What is 1 + 1? [/INST]\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate with conservative parameters\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.0,  # No randomness at all\n",
        "        do_sample=False,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "# Decode full response\n",
        "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print just the model's answer by removing the input prompt\n",
        "answer = full_response[len(input_text):] if full_response.startswith(input_text) else full_response\n",
        "print(f\"Model answer: {answer}\")"
      ],
      "metadata": {
        "id": "cTEoK8s73aSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Print versions for debugging\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "\n",
        "# Load the model you specifically want to use\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "try:\n",
        "    # Force 2-bit specific configurations\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True  # May be needed for custom quantization\n",
        "    )\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Try using a Llama-specific prompt format\n",
        "    prompt = \"\"\"<|begin_of_text|><|system|>\n",
        "You are a helpful AI assistant. Answer the question accurately and briefly.\n",
        "<|end_of_turn|>\n",
        "<|user|>\n",
        "What is 1 + 1?\n",
        "<|end_of_turn|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "    # Generate with minimal parameters\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():  # Use inference mode for generation\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=20,\n",
        "            temperature=0.0,  # No sampling\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.0  # Default, no penalty\n",
        "        )\n",
        "\n",
        "    # Try different decoding approaches\n",
        "    # Approach 1: Full decode\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Full response: {full_response}\")\n",
        "\n",
        "    # Approach 2: Decode only new tokens\n",
        "    new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "    new_response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "    print(f\"New tokens only: {new_response}\")\n",
        "\n",
        "    # Approach 3: Try batched decoding\n",
        "    batched_response = \"\"\n",
        "    for i in range(len(new_tokens)):\n",
        "        token_text = tokenizer.decode([new_tokens[i]], skip_special_tokens=False)\n",
        "        batched_response += token_text\n",
        "    print(f\"Batched decode: {batched_response}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or running model: {str(e)}\")\n",
        "    print(\"Consider trying a different model or quantization level\")"
      ],
      "metadata": {
        "id": "49MAmF-l4SGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is based off of the generation script in https://github.com/chu-tianxiang/QuIP-for-all\n",
        "import os\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from model.cache_utils import StaticCache\n",
        "\n",
        "from lib.utils.unsafe_import import model_from_hf_path\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "def multinomial_sample_one_no_sync(\n",
        "        probs_sort\n",
        "):  # Does multinomial sampling without a cuda synchronization\n",
        "    q = torch.empty_like(probs_sort).exponential_(1)\n",
        "    return torch.argmax(probs_sort / q, dim=-1,\n",
        "                        keepdim=True).to(dtype=torch.int)\n",
        "\n",
        "\n",
        "def logits_to_probs(logits,\n",
        "                    temperature: float = 1.0,\n",
        "                    top_k: Optional[int] = None):\n",
        "    logits = logits / max(temperature, 1e-5)\n",
        "\n",
        "    if top_k is not None:\n",
        "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
        "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    return probs\n",
        "\n",
        "@torch.compile\n",
        "def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n",
        "    probs = logits_to_probs(logits[:, -1], temperature, top_k)\n",
        "    idx_next = multinomial_sample_one_no_sync(probs)\n",
        "    return idx_next, probs\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_one_tokens(model, cur_token, past_kv, cache_position):\n",
        "    logits = model(cur_token,\n",
        "                   past_key_values=past_kv,\n",
        "                   cache_position=cache_position)[0]\n",
        "    new_token = sample(logits, temperature=0.6, top_k=5)[0]\n",
        "    return new_token, logits\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, text, max_new_tokens, top_k, callback, past_kv):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
        "    batch_size, seq_length = inputs[\"input_ids\"].shape\n",
        "    cache_position = torch.arange(seq_length, device=0)\n",
        "    generated_ids = torch.zeros(batch_size,\n",
        "                                seq_length + max_new_tokens,\n",
        "                                dtype=torch.int,\n",
        "                                device=0)\n",
        "    generated_ids[:, cache_position] = inputs[\"input_ids\"].to(0).int()\n",
        "    logits = model(**inputs,\n",
        "                   past_key_values=past_kv,\n",
        "                   cache_position=cache_position)[0]\n",
        "\n",
        "    next_token, _ = sample(logits, top_k=top_k)\n",
        "\n",
        "    generated_ids[:, seq_length] = next_token\n",
        "    callback(next_token)\n",
        "\n",
        "    cache_position = torch.tensor([seq_length + 1], device=0)\n",
        "    decode_time = time.time()\n",
        "    for _ in range(1, max_new_tokens):\n",
        "        with torch.backends.cuda.sdp_kernel(enable_flash=True,\n",
        "                                            enable_mem_efficient=False,\n",
        "                                            enable_math=True):\n",
        "            next_token, logits = decode_one_tokens(model, next_token.clone(),\n",
        "                                                   past_kv, cache_position)\n",
        "        generated_ids[:, cache_position] = next_token.int()\n",
        "        callback(next_token)\n",
        "        cache_position += 1\n",
        "    torch.cuda.synchronize()\n",
        "    decode_time = time.time() - decode_time\n",
        "\n",
        "    text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    return generated_ids, text, max_new_tokens / decode_time\n",
        "\n",
        "\n",
        "def llama_arg_fn(output, args, kwargs):\n",
        "    return (output[0], *args[1:]), kwargs\n",
        "\n",
        "def get_emb(args, kwargs):\n",
        "    return args[0]\n",
        "\n",
        "\n",
        "from lib.utils import shard_model as sm, clean\n",
        "\n",
        "def main(hf_path, compile, interactive, max_tokens, top_k):\n",
        "    device = \"cuda\"\n",
        "    model, model_str = model_from_hf_path(hf_path)\n",
        "\n",
        "    sharded = False\n",
        "\n",
        "    if 'llama' in model_str.lower():\n",
        "        n_shards = model.lm_head.weight.device.index + 1\n",
        "        if n_shards > 1:\n",
        "            sharded = True\n",
        "            del model.model.layers\n",
        "            clean()\n",
        "            cpumodel, _ = model_from_hf_path(hf_path, device_map='cpu')\n",
        "            nlayers = len(cpumodel.model.layers)\n",
        "            shards = [torch.nn.ModuleList([]) for _ in range(n_shards)]\n",
        "            layer_device_map = []\n",
        "            for i in range(n_shards):\n",
        "                for j in range(int(nlayers * i / n_shards),\n",
        "                               int(nlayers * (i + 1) / n_shards)):\n",
        "                    shards[i].append(cpumodel.model.layers[j])\n",
        "                    layer_device_map.append(i)\n",
        "                shards[i] = {'device': i, 'arg_fn': llama_arg_fn, 'shard': shards[i]}\n",
        "            model.model.layers = [sm.ShardDecoderLayers(shards, model.lm_head.weight.dtype)]\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    if not sharded:\n",
        "        past_kv = StaticCache(model.config,\n",
        "                              1,\n",
        "                              2*args.max_new_tokens,\n",
        "                              device=0,\n",
        "                              dtype=model.dtype)\n",
        "    else:\n",
        "        past_kv = StaticCache(model.config,\n",
        "                              1,\n",
        "                              2*args.max_new_tokens,\n",
        "                              layer_device_map=layer_device_map,\n",
        "                              dtype=model.dtype)\n",
        "\n",
        "    text = \"This is a test of this large language model\"\n",
        "    callback = lambda x: x\n",
        "    ids, text, _ = generate(model, tokenizer, text,\n",
        "                            8, top_k, callback, past_kv)\n",
        "\n",
        "    if compile:\n",
        "        print('Capturing CUDA graphs, may take some time. If you are running a model over multiple GPUs, the first generation will be very slow due to compiling the model.')\n",
        "        if not sharded:\n",
        "            global decode_one_tokens\n",
        "            decode_one_tokens = torch.compile(decode_one_tokens,\n",
        "                                              mode=\"max-autotune\",\n",
        "                                              fullgraph=True)\n",
        "        else:\n",
        "            for shard in model.model.layers[0].shards:\n",
        "                shard.forward = torch.compile(shard.forward,\n",
        "                                              mode='max-autotune',\n",
        "                                              fullgraph=True)\n",
        "\n",
        "\n",
        "\n",
        "    text = \"This is a test of this large language model\"\n",
        "    ids, text, _ = generate(model, tokenizer, text,\n",
        "                            16, top_k, callback, past_kv)\n",
        "\n",
        "    while True:\n",
        "        prompt = input(\"What is your prompt? \")\n",
        "        if prompt == 'quit':\n",
        "            exit()\n",
        "        if tokenizer.chat_template is not None:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            text = tokenizer.apply_chat_template(messages,\n",
        "                                                 tokenize=False,\n",
        "                                                 add_generation_prompt=True)\n",
        "        else:\n",
        "            text = prompt\n",
        "        buffer = []\n",
        "        period_id = tokenizer.encode('.')[-1]\n",
        "        done_generating = False\n",
        "\n",
        "        def callback(x):\n",
        "            nonlocal done_generating\n",
        "            if done_generating:\n",
        "                return\n",
        "            buffer.append(tokenizer.decode([period_id] + x[0].tolist())[1:])\n",
        "            if x[0].item() == tokenizer.eos_token_id:\n",
        "                done_generating = True\n",
        "            if len(buffer) == 4 or done_generating:\n",
        "                print(''.join(buffer), end='', flush=True)\n",
        "                buffer.clear()\n",
        "\n",
        "        if not interactive:\n",
        "            callback = lambda x: x\n",
        "        ids, text, decode_tps = generate(model, tokenizer, text,\n",
        "                                         max_tokens, top_k, callback, past_kv)\n",
        "        if not interactive:\n",
        "            print(text)\n",
        "\n",
        "        print(\n",
        "            f\"\\nDecoding throughput: {decode_tps:.02f} tokens/sec. Includes tokens generated after the EOS token.\\n\\n\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='Your CLI description.')\n",
        "\n",
        "    parser.add_argument('--hf_path', type=str, help=\"Path to checkpoint\")\n",
        "    parser.add_argument('--streaming',\n",
        "                        action='store_true',\n",
        "                        help='Whether to launch in stream mode')\n",
        "    parser.add_argument('--max_new_tokens',\n",
        "                        type=int,\n",
        "                        default=512,\n",
        "                        help='Maximum number of new tokens.')\n",
        "    parser.add_argument('--top_k',\n",
        "                        type=int,\n",
        "                        default=32,\n",
        "                        help='Top-k for sampling.')\n",
        "    parser.add_argument('--no_compile',\n",
        "                        action='store_true',\n",
        "                        help='Whether to compile the model.')\n",
        "    parser.add_argument('--disable_tf32',\n",
        "                        action='store_true',\n",
        "                        help='Whether to disable TF32 for FP32 matmuls.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not args.disable_tf32:\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    main(args.hf_path, not args.no_compile, args.streaming,\n",
        "         args.max_new_tokens, args.top_k)"
      ],
      "metadata": {
        "id": "CVrpwOYY44Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت المكتبات اللازمة إذا لم تكن موجودة\n",
        "# pip install torch transformers\n",
        "\n",
        "# احفظ الكود في ملف باسم generate.py\n",
        "\n",
        "# ثم قم بتشغيله بهذا الأمر\n",
        "# python generate.py --hf_path \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\" --streaming --max_new_tokens 512"
      ],
      "metadata": {
        "id": "VjVqSMi841W-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}