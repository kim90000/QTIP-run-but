{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56ba32bc71794d05b6a101a534cad0c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a756b8d2a8634108831377c2515822d3",
              "IPY_MODEL_fea8a2dd073c4ab686bba3cfebfd6890",
              "IPY_MODEL_2719d507d2a94dc996080a944d9e2574"
            ],
            "layout": "IPY_MODEL_ba383158e7444c8db7f177248c55a036"
          }
        },
        "a756b8d2a8634108831377c2515822d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2487c7185cf94f79b2c249e36c87d72d",
            "placeholder": "​",
            "style": "IPY_MODEL_822aa7c1540d48cdb13216ea72c42997",
            "value": "config.json: 100%"
          }
        },
        "fea8a2dd073c4ab686bba3cfebfd6890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_700c3de167914c1f979b0f2c9ffa3ab8",
            "max": 1117,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f8a65c9d3f049888a8db3d6fbecd24c",
            "value": 1117
          }
        },
        "2719d507d2a94dc996080a944d9e2574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47aa155913b944bebae5c2c56f34ac68",
            "placeholder": "​",
            "style": "IPY_MODEL_74964cfe08e04c6eb1fc1ecc2a1afb45",
            "value": " 1.12k/1.12k [00:00&lt;00:00, 79.5kB/s]"
          }
        },
        "ba383158e7444c8db7f177248c55a036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2487c7185cf94f79b2c249e36c87d72d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822aa7c1540d48cdb13216ea72c42997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "700c3de167914c1f979b0f2c9ffa3ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8a65c9d3f049888a8db3d6fbecd24c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47aa155913b944bebae5c2c56f34ac68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74964cfe08e04c6eb1fc1ecc2a1afb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce7069105e1b4048b39b2d6abae3002e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62a4eee1fdd4467abc5c07c6f0e9b345",
              "IPY_MODEL_a33ce85feca442c9a56dddd15536018a",
              "IPY_MODEL_38ba2838a7b147cba999491f31b366fc"
            ],
            "layout": "IPY_MODEL_4448cbe40b71410fbd008a7fdb15429a"
          }
        },
        "62a4eee1fdd4467abc5c07c6f0e9b345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_623747c6b40240e09f1923f88ac5f506",
            "placeholder": "​",
            "style": "IPY_MODEL_5b743d32f79646ca8aea03f7392ab7ec",
            "value": "model.safetensors:  90%"
          }
        },
        "a33ce85feca442c9a56dddd15536018a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a71b4ec730e41cc894ea081d8e506e1",
            "max": 3852517272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6ddee29c2e54dfb91bb7b139c112b1f",
            "value": 3449815040
          }
        },
        "38ba2838a7b147cba999491f31b366fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a67dd6a9a4bb4e84a29849cf7a6c2d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_7ef0f6fe588d4998ac4a2f9736337fad",
            "value": " 3.45G/3.85G [02:25&lt;00:16, 23.9MB/s]"
          }
        },
        "4448cbe40b71410fbd008a7fdb15429a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623747c6b40240e09f1923f88ac5f506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b743d32f79646ca8aea03f7392ab7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a71b4ec730e41cc894ea081d8e506e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6ddee29c2e54dfb91bb7b139c112b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a67dd6a9a4bb4e84a29849cf7a6c2d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ef0f6fe588d4998ac4a2f9736337fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb23e5420f1445dc81454dc6802bf464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7231090105c44f5bb625c00c4abd7e64",
              "IPY_MODEL_029c0073b9c345c28fff5615bd3a83bb",
              "IPY_MODEL_8d756722d98046dba33d7ee6de4c5ff3"
            ],
            "layout": "IPY_MODEL_8870052f96214f8c8ab1e15d06a776f3"
          }
        },
        "7231090105c44f5bb625c00c4abd7e64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bff70bd8f29b435ea8052318220b00bb",
            "placeholder": "​",
            "style": "IPY_MODEL_4d4752393e8c45da859913d977817505",
            "value": "model.safetensors: 100%"
          }
        },
        "029c0073b9c345c28fff5615bd3a83bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a77247af9314a2f9785354800ab87cc",
            "max": 3852517272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_615a74708512447dacce8ba002a67588",
            "value": 3852517272
          }
        },
        "8d756722d98046dba33d7ee6de4c5ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32806b92aef84b86bb805d503c28d0e8",
            "placeholder": "​",
            "style": "IPY_MODEL_ec567a599780408186a54eeaf79219a1",
            "value": " 3.85G/3.85G [00:16&lt;00:00, 23.7MB/s]"
          }
        },
        "8870052f96214f8c8ab1e15d06a776f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bff70bd8f29b435ea8052318220b00bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d4752393e8c45da859913d977817505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a77247af9314a2f9785354800ab87cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615a74708512447dacce8ba002a67588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32806b92aef84b86bb805d503c28d0e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec567a599780408186a54eeaf79219a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_rFKebPOIQw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLnyKr7EOWpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token ْْْْْْْْXXXXXXXXXXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ht8cDETO3_0",
        "outputId": "1ebda212-4ac0-4f42-e101-629790a72fcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIaV2FXiPuwt",
        "outputId": "2e6cffc3-f316-4752-bcd4-666b0d45e1e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install huggingface_hub[cli]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gMhbEBRFRZVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!huggingface-cli login --token YOUR_TOKEN"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "y8pKVpw0RfLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"ما هي أهمية الذكاء الاصطناعي في المستقبل؟\"\n",
        "\n",
        "# تنفيذ الاستدلال وتوليد النص\n",
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=200,  # الحد الأقصى لطول النص المُولد\n",
        "    temperature=0.7,  # التحكم في العشوائية\n",
        "    top_k=50,  # اختيار أفضل 50 خيارًا للتوليد\n",
        "    top_p=0.9,  # فلترة الخيارات باستخدام النسبة التراكمية\n",
        "    repetition_penalty=1.2,  # تقليل تكرار الكلمات\n",
        ")\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "56ba32bc71794d05b6a101a534cad0c8",
            "a756b8d2a8634108831377c2515822d3",
            "fea8a2dd073c4ab686bba3cfebfd6890",
            "2719d507d2a94dc996080a944d9e2574",
            "ba383158e7444c8db7f177248c55a036",
            "2487c7185cf94f79b2c249e36c87d72d",
            "822aa7c1540d48cdb13216ea72c42997",
            "700c3de167914c1f979b0f2c9ffa3ab8",
            "6f8a65c9d3f049888a8db3d6fbecd24c",
            "47aa155913b944bebae5c2c56f34ac68",
            "74964cfe08e04c6eb1fc1ecc2a1afb45",
            "ce7069105e1b4048b39b2d6abae3002e",
            "62a4eee1fdd4467abc5c07c6f0e9b345",
            "a33ce85feca442c9a56dddd15536018a",
            "38ba2838a7b147cba999491f31b366fc",
            "4448cbe40b71410fbd008a7fdb15429a",
            "623747c6b40240e09f1923f88ac5f506",
            "5b743d32f79646ca8aea03f7392ab7ec",
            "6a71b4ec730e41cc894ea081d8e506e1",
            "b6ddee29c2e54dfb91bb7b139c112b1f",
            "a67dd6a9a4bb4e84a29849cf7a6c2d0f",
            "7ef0f6fe588d4998ac4a2f9736337fad",
            "bb23e5420f1445dc81454dc6802bf464",
            "7231090105c44f5bb625c00c4abd7e64",
            "029c0073b9c345c28fff5615bd3a83bb",
            "8d756722d98046dba33d7ee6de4c5ff3",
            "8870052f96214f8c8ab1e15d06a776f3",
            "bff70bd8f29b435ea8052318220b00bb",
            "4d4752393e8c45da859913d977817505",
            "5a77247af9314a2f9785354800ab87cc",
            "615a74708512447dacce8ba002a67588",
            "32806b92aef84b86bb805d503c28d0e8",
            "ec567a599780408186a54eeaf79219a1"
          ]
        },
        "id": "zq2nOTsuOWsr",
        "outputId": "ff633cfb-3cb8-4029-c8fd-42445e89916a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ba32bc71794d05b6a101a534cad0c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce7069105e1b4048b39b2d6abae3002e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:  90%|########9 | 3.45G/3.85G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb23e5420f1445dc81454dc6802bf464"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "vCu9HbfdkNUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "Write a love poem.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot who always responds in polite manner\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    max_new_tokens=55,\n",
        ")\n",
        "text=outputs[0][\"generated_text\"][-1]\n",
        "print(text['content'])\n"
      ],
      "metadata": {
        "id": "kofAEGbyOgcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "Write a love poem.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot who always responds in polite manner\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    prompt,\n",
        "    max_new_tokens=55,\n",
        ")\n",
        "text=outputs[0][\"generated_text\"][-1]\n",
        "print(text['content'])"
      ],
      "metadata": {
        "id": "n0zw-DW-lObV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a"
      ],
      "metadata": {
        "id": "k9aVQBQnms6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fahd"
      ],
      "metadata": {
        "id": "sX4Yv0C2nz5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "prompt=\"\"\"\n",
        "Write a love poem with exactly 50 words.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot who always responds in polite manner\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "text=outputs[0][\"generated_text\"][-1]\n",
        "print(text['content'])\n"
      ],
      "metadata": {
        "id": "B-zkaQHxlGfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "# تحميل الـ pipeline\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# النص المُدخل\n",
        "prompt = \"Write a love poem with exactly 50 words.\"\n",
        "\n",
        "# استدعاء النموذج لتوليد النص\n",
        "outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,  # لجعل الاستجابة أكثر تنوعًا\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(outputs[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "tJD9-GMzny52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "# تحديد معرف النموذج\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "# إنشاء خط الأنابيب (Pipeline) لمعالجة النصوص\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",  # يستخدم تلقائيًا الـ GPU إذا كان متاحًا\n",
        ")\n",
        "\n",
        "# إدخال نص كمثال\n",
        "prompt = \"what is ai?\"\n",
        "\n",
        "# تنفيذ الاستدلال وتوليد النص\n",
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=200,  # الحد الأقصى لطول النص المُولد\n",
        "    temperature=0.7,  # التحكم في العشوائية\n",
        "    top_k=50,  # اختيار أفضل 50 خيارًا للتوليد\n",
        "    top_p=0.9,  # فلترة الخيارات باستخدام النسبة التراكمية\n",
        "    repetition_penalty=1.2,  # تقليل تكرار الكلمات\n",
        ")\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "4bI6VqYQZXAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what is ai?\"\n",
        "\n",
        "# تنفيذ الاستدلال وتوليد النص\n",
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=50,  # الحد الأقصى لطول النص المُولد\n",
        "    temperature=0.7,  # التحكم في العشوائية\n",
        "    top_k=50,  # اختيار أفضل 50 خيارًا للتوليد\n",
        "    top_p=0.9,  # فلترة الخيارات باستخدام النسبة التراكمية\n",
        "    repetition_penalty=1.2,  # تقليل تكرار الكلمات\n",
        ")\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "aoD_dDVyZgg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=10,\n",
        "    truncation=True,  # إضافة هذا الخيار لتجنب التحذير\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        ")\n"
      ],
      "metadata": {
        "id": "L54dPKaDaUBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHuwsJCMbyNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ywJZbtEkbyLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import numpy\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",  # يستخدم تلقائيًا الـ GPU إذا كان متاحًا\n",
        ")\n",
        "prompt = \"what is ai?\"\n",
        "output = pipeline(\n",
        "    prompt,\n",
        "    max_length=10,\n",
        "    truncation=True,  # إضافة هذا الخيار لتجنب التحذير\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2,\n",
        ")\n",
        "print(output[0][\"generated_text\"])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hfq0TkqZbyIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "text = \"What is AI?\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "iZIuY2w5dKSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "prompt = \"What is python?\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "output = model.generate(input_ids, max_length=50, do_sample=True, temperature=0.7, top_k=50, top_p=0.9)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "6WbtTyLbggaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is python?\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, do_sample=True, temperature=0.7, top_k=50, top_p=0.9)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "EN4xT0rigkuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TBW1-TysrbM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Qm3JfOdrbJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ULnhcE1YrbGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uxf_iaMarbBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        " model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        " model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        " text = \"What is AI?\"\n",
        " inputs = tokenizer(text, return_tensors=\"pt\")\n",
        " outputs = model.generate(**inputs, max_length=50)\n",
        " print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "dif3JVa7ra9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        " from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        " model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        " tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        " model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        " text = \"What is AI?\"\n",
        " inputs = tokenizer(text, return_tensors=\"pt\")\n",
        " outputs = model.generate(**inputs, max_length=50)\n",
        " print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JmKGFQJr2qk",
        "outputId": "c8e57f24-4fff-458b-b489-e2d475c7c41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f"
      ],
      "metadata": {
        "id": "P2Ni2Bt2r2o-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"1 + 1 = ?\"\n",
        "\n",
        "\n",
        "max_new_tokens\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=5, do_sample=True)"
      ],
      "metadata": {
        "id": "a2-her0er-n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "text = \"What is AI?\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=50, do_sample=True)  # Added do_sample=True\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MrEV4Zs0tuPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text = \"1 + 1 = ?\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=9, do_sample=True\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "f2mmgl2Swtfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sont"
      ],
      "metadata": {
        "id": "Lw5eCh05xJ5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "text = \"1 + 1 = ?\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=20,  # Increased max_length for a more complete response\n",
        "    do_sample=False  # Set to False for deterministic output\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "CFoQN_LhxJPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load model with proper configurations\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "# Load tokenizer with chat template if available\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Format input using proper prompt template for instruction models\n",
        "# Many LLaMA models expect a specific format\n",
        "prompt = \"\"\"<|begin_of_text|><|user|>\n",
        "What is 1 + 1?\n",
        "<|end_of_turn|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "# Generate with more careful parameters\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.1,  # Lower temperature for more deterministic output\n",
        "    do_sample=True,   # Still allow some sampling with low temperature\n",
        "    top_p=0.95,       # Nucleus sampling\n",
        "    repetition_penalty=1.2,  # Penalize repetition\n",
        "    pad_token_id=tokenizer.eos_token_id  # Use EOS as PAD\n",
        ")\n",
        "\n",
        "# Decode only the new tokens, not the input prompt\n",
        "answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "print(f\"Input prompt: {prompt}\")\n",
        "print(f\"Generated answer: {answer}\")"
      ],
      "metadata": {
        "id": "o0t9baGO0VHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Try a different, more standard model\n",
        "model_id = \"meta-llama/Llama-3-8b-instruct\"  # Use the official Meta model if you have access\n",
        "# OR use a different, reliable model:\n",
        "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Load with minimal modifications\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16  # More stable than float16\n",
        ")\n",
        "\n",
        "# Use the model's built-in chat template if available\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is 1 + 1?\"}\n",
        "]\n",
        "\n",
        "# Format with chat template\n",
        "if hasattr(tokenizer, 'apply_chat_template'):\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "else:\n",
        "    # Fallback for older models without chat template\n",
        "    input_text = f\"<s>[INST] What is 1 + 1? [/INST]\"\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate with conservative parameters\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=50,\n",
        "        temperature=0.0,  # No randomness at all\n",
        "        do_sample=False,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "# Decode full response\n",
        "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print just the model's answer by removing the input prompt\n",
        "answer = full_response[len(input_text):] if full_response.startswith(input_text) else full_response\n",
        "print(f\"Model answer: {answer}\")"
      ],
      "metadata": {
        "id": "cTEoK8s73aSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Print versions for debugging\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "\n",
        "# Load the model you specifically want to use\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "\n",
        "try:\n",
        "    # Force 2-bit specific configurations\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True  # May be needed for custom quantization\n",
        "    )\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Try using a Llama-specific prompt format\n",
        "    prompt = \"\"\"<|begin_of_text|><|system|>\n",
        "You are a helpful AI assistant. Answer the question accurately and briefly.\n",
        "<|end_of_turn|>\n",
        "<|user|>\n",
        "What is 1 + 1?\n",
        "<|end_of_turn|>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "    # Generate with minimal parameters\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode():  # Use inference mode for generation\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=20,\n",
        "            temperature=0.0,  # No sampling\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.0  # Default, no penalty\n",
        "        )\n",
        "\n",
        "    # Try different decoding approaches\n",
        "    # Approach 1: Full decode\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Full response: {full_response}\")\n",
        "\n",
        "    # Approach 2: Decode only new tokens\n",
        "    new_tokens = outputs[0][inputs.input_ids.shape[1]:]\n",
        "    new_response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "    print(f\"New tokens only: {new_response}\")\n",
        "\n",
        "    # Approach 3: Try batched decoding\n",
        "    batched_response = \"\"\n",
        "    for i in range(len(new_tokens)):\n",
        "        token_text = tokenizer.decode([new_tokens[i]], skip_special_tokens=False)\n",
        "        batched_response += token_text\n",
        "    print(f\"Batched decode: {batched_response}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or running model: {str(e)}\")\n",
        "    print(\"Consider trying a different model or quantization level\")"
      ],
      "metadata": {
        "id": "49MAmF-l4SGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is based off of the generation script in https://github.com/chu-tianxiang/QuIP-for-all\n",
        "import os\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from model.cache_utils import StaticCache\n",
        "\n",
        "from lib.utils.unsafe_import import model_from_hf_path\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "\n",
        "def multinomial_sample_one_no_sync(\n",
        "        probs_sort\n",
        "):  # Does multinomial sampling without a cuda synchronization\n",
        "    q = torch.empty_like(probs_sort).exponential_(1)\n",
        "    return torch.argmax(probs_sort / q, dim=-1,\n",
        "                        keepdim=True).to(dtype=torch.int)\n",
        "\n",
        "\n",
        "def logits_to_probs(logits,\n",
        "                    temperature: float = 1.0,\n",
        "                    top_k: Optional[int] = None):\n",
        "    logits = logits / max(temperature, 1e-5)\n",
        "\n",
        "    if top_k is not None:\n",
        "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
        "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    return probs\n",
        "\n",
        "@torch.compile\n",
        "def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n",
        "    probs = logits_to_probs(logits[:, -1], temperature, top_k)\n",
        "    idx_next = multinomial_sample_one_no_sync(probs)\n",
        "    return idx_next, probs\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def decode_one_tokens(model, cur_token, past_kv, cache_position):\n",
        "    logits = model(cur_token,\n",
        "                   past_key_values=past_kv,\n",
        "                   cache_position=cache_position)[0]\n",
        "    new_token = sample(logits, temperature=0.6, top_k=5)[0]\n",
        "    return new_token, logits\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, text, max_new_tokens, top_k, callback, past_kv):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
        "    batch_size, seq_length = inputs[\"input_ids\"].shape\n",
        "    cache_position = torch.arange(seq_length, device=0)\n",
        "    generated_ids = torch.zeros(batch_size,\n",
        "                                seq_length + max_new_tokens,\n",
        "                                dtype=torch.int,\n",
        "                                device=0)\n",
        "    generated_ids[:, cache_position] = inputs[\"input_ids\"].to(0).int()\n",
        "    logits = model(**inputs,\n",
        "                   past_key_values=past_kv,\n",
        "                   cache_position=cache_position)[0]\n",
        "\n",
        "    next_token, _ = sample(logits, top_k=top_k)\n",
        "\n",
        "    generated_ids[:, seq_length] = next_token\n",
        "    callback(next_token)\n",
        "\n",
        "    cache_position = torch.tensor([seq_length + 1], device=0)\n",
        "    decode_time = time.time()\n",
        "    for _ in range(1, max_new_tokens):\n",
        "        with torch.backends.cuda.sdp_kernel(enable_flash=True,\n",
        "                                            enable_mem_efficient=False,\n",
        "                                            enable_math=True):\n",
        "            next_token, logits = decode_one_tokens(model, next_token.clone(),\n",
        "                                                   past_kv, cache_position)\n",
        "        generated_ids[:, cache_position] = next_token.int()\n",
        "        callback(next_token)\n",
        "        cache_position += 1\n",
        "    torch.cuda.synchronize()\n",
        "    decode_time = time.time() - decode_time\n",
        "\n",
        "    text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    return generated_ids, text, max_new_tokens / decode_time\n",
        "\n",
        "\n",
        "def llama_arg_fn(output, args, kwargs):\n",
        "    return (output[0], *args[1:]), kwargs\n",
        "\n",
        "def get_emb(args, kwargs):\n",
        "    return args[0]\n",
        "\n",
        "\n",
        "from lib.utils import shard_model as sm, clean\n",
        "\n",
        "def main(hf_path, compile, interactive, max_tokens, top_k):\n",
        "    device = \"cuda\"\n",
        "    model, model_str = model_from_hf_path(hf_path)\n",
        "\n",
        "    sharded = False\n",
        "\n",
        "    if 'llama' in model_str.lower():\n",
        "        n_shards = model.lm_head.weight.device.index + 1\n",
        "        if n_shards > 1:\n",
        "            sharded = True\n",
        "            del model.model.layers\n",
        "            clean()\n",
        "            cpumodel, _ = model_from_hf_path(hf_path, device_map='cpu')\n",
        "            nlayers = len(cpumodel.model.layers)\n",
        "            shards = [torch.nn.ModuleList([]) for _ in range(n_shards)]\n",
        "            layer_device_map = []\n",
        "            for i in range(n_shards):\n",
        "                for j in range(int(nlayers * i / n_shards),\n",
        "                               int(nlayers * (i + 1) / n_shards)):\n",
        "                    shards[i].append(cpumodel.model.layers[j])\n",
        "                    layer_device_map.append(i)\n",
        "                shards[i] = {'device': i, 'arg_fn': llama_arg_fn, 'shard': shards[i]}\n",
        "            model.model.layers = [sm.ShardDecoderLayers(shards, model.lm_head.weight.dtype)]\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    if not sharded:\n",
        "        past_kv = StaticCache(model.config,\n",
        "                              1,\n",
        "                              2*args.max_new_tokens,\n",
        "                              device=0,\n",
        "                              dtype=model.dtype)\n",
        "    else:\n",
        "        past_kv = StaticCache(model.config,\n",
        "                              1,\n",
        "                              2*args.max_new_tokens,\n",
        "                              layer_device_map=layer_device_map,\n",
        "                              dtype=model.dtype)\n",
        "\n",
        "    text = \"This is a test of this large language model\"\n",
        "    callback = lambda x: x\n",
        "    ids, text, _ = generate(model, tokenizer, text,\n",
        "                            8, top_k, callback, past_kv)\n",
        "\n",
        "    if compile:\n",
        "        print('Capturing CUDA graphs, may take some time. If you are running a model over multiple GPUs, the first generation will be very slow due to compiling the model.')\n",
        "        if not sharded:\n",
        "            global decode_one_tokens\n",
        "            decode_one_tokens = torch.compile(decode_one_tokens,\n",
        "                                              mode=\"max-autotune\",\n",
        "                                              fullgraph=True)\n",
        "        else:\n",
        "            for shard in model.model.layers[0].shards:\n",
        "                shard.forward = torch.compile(shard.forward,\n",
        "                                              mode='max-autotune',\n",
        "                                              fullgraph=True)\n",
        "\n",
        "\n",
        "\n",
        "    text = \"This is a test of this large language model\"\n",
        "    ids, text, _ = generate(model, tokenizer, text,\n",
        "                            16, top_k, callback, past_kv)\n",
        "\n",
        "    while True:\n",
        "        prompt = input(\"What is your prompt? \")\n",
        "        if prompt == 'quit':\n",
        "            exit()\n",
        "        if tokenizer.chat_template is not None:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "            text = tokenizer.apply_chat_template(messages,\n",
        "                                                 tokenize=False,\n",
        "                                                 add_generation_prompt=True)\n",
        "        else:\n",
        "            text = prompt\n",
        "        buffer = []\n",
        "        period_id = tokenizer.encode('.')[-1]\n",
        "        done_generating = False\n",
        "\n",
        "        def callback(x):\n",
        "            nonlocal done_generating\n",
        "            if done_generating:\n",
        "                return\n",
        "            buffer.append(tokenizer.decode([period_id] + x[0].tolist())[1:])\n",
        "            if x[0].item() == tokenizer.eos_token_id:\n",
        "                done_generating = True\n",
        "            if len(buffer) == 4 or done_generating:\n",
        "                print(''.join(buffer), end='', flush=True)\n",
        "                buffer.clear()\n",
        "\n",
        "        if not interactive:\n",
        "            callback = lambda x: x\n",
        "        ids, text, decode_tps = generate(model, tokenizer, text,\n",
        "                                         max_tokens, top_k, callback, past_kv)\n",
        "        if not interactive:\n",
        "            print(text)\n",
        "\n",
        "        print(\n",
        "            f\"\\nDecoding throughput: {decode_tps:.02f} tokens/sec. Includes tokens generated after the EOS token.\\n\\n\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='Your CLI description.')\n",
        "\n",
        "    parser.add_argument('--hf_path', type=str, help=\"Path to checkpoint\")\n",
        "    parser.add_argument('--streaming',\n",
        "                        action='store_true',\n",
        "                        help='Whether to launch in stream mode')\n",
        "    parser.add_argument('--max_new_tokens',\n",
        "                        type=int,\n",
        "                        default=512,\n",
        "                        help='Maximum number of new tokens.')\n",
        "    parser.add_argument('--top_k',\n",
        "                        type=int,\n",
        "                        default=32,\n",
        "                        help='Top-k for sampling.')\n",
        "    parser.add_argument('--no_compile',\n",
        "                        action='store_true',\n",
        "                        help='Whether to compile the model.')\n",
        "    parser.add_argument('--disable_tf32',\n",
        "                        action='store_true',\n",
        "                        help='Whether to disable TF32 for FP32 matmuls.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if not args.disable_tf32:\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    main(args.hf_path, not args.no_compile, args.streaming,\n",
        "         args.max_new_tokens, args.top_k)"
      ],
      "metadata": {
        "id": "CVrpwOYY44Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت المكتبات اللازمة إذا لم تكن موجودة\n",
        "# pip install torch transformers\n",
        "\n",
        "# احفظ الكود في ملف باسم generate.py\n",
        "\n",
        "# ثم قم بتشغيله بهذا الأمر\n",
        "# python generate.py --hf_path \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\" --streaming --max_new_tokens 512"
      ],
      "metadata": {
        "id": "VjVqSMi841W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C:\\Users\\TARGET STORE\\.cache\\huggingface\\hub\\models--relaxml--Llama-3.1-8b-Instruct-QTIP-2Bit"
      ],
      "metadata": {
        "id": "gyLlI0Wa5z7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python generate.py --hf_path \"C:\\Users\\TARGET STORE\\.cache\\huggingface\\hub\\models--relaxml--Llama-3.1-8b-Instruct-QTIP-2Bit\\snapshots\\b67a9863702e87c44eb197250ae90ba910e75a66\" --streaming --max_new_tokens 51"
      ],
      "metadata": {
        "id": "ix6deh6C53yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C:\\Users\\TARGET STORE\\.cache\\huggingface\\hub\\models--relaxml--Llama-3.1-8b-Instruct-QTIP-2Bit\\snapshots\\b67a9863702e87c44eb197250ae90ba910e75a66"
      ],
      "metadata": {
        "id": "Y2J17Pwc6Fog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_generate.py"
      ],
      "metadata": {
        "id": "aYcGjHzB6rmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python example_generate.py --hf_path \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\" --streaming --max_new_tokens 512"
      ],
      "metadata": {
        "id": "o9yNjrwV6ueA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "son\n"
      ],
      "metadata": {
        "id": "8THhlicV9vWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ش"
      ],
      "metadata": {
        "id": "8qF8E5IC-a9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用设备: {device}\")\n",
        "\n",
        "# 加载模型和分词器\n",
        "model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "print(f\"正在加载模型 {model_id}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        ")\n",
        "\n",
        "# 准备用于生成的函数\n",
        "def generate_text(prompt, max_length=100):\n",
        "    \"\"\"生成文本的函数\"\"\"\n",
        "    # 如果模型有聊天模板，使用它\n",
        "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        # 否则使用基本的提示格式\n",
        "        input_text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # 开始生成\n",
        "    print(\"\\n开始生成...\\n\")\n",
        "    print(f\"提示: {prompt}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 渐进式生成\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        streamer=None  # 如果你想使用流式生成，可以添加TextStreamer\n",
        "    )\n",
        "\n",
        "    # 解码生成的ID\n",
        "    output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # 尝试分离出模型的回复部分\n",
        "    if input_text in output:\n",
        "        response = output[len(input_text):]\n",
        "    else:\n",
        "        response = output\n",
        "\n",
        "    print(response)\n",
        "    print(\"-\" * 50)\n",
        "    return response\n",
        "\n",
        "# 交互式命令行界面\n",
        "def interactive_cli():\n",
        "    \"\"\"交互式命令行界面\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"模型已加载，可以开始生成文本\")\n",
        "    print(\"输入 'quit' 或 'exit' 退出\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    while True:\n",
        "        prompt = input(\"\\n请输入提示: \")\n",
        "        if prompt.lower() in ['quit', 'exit']:\n",
        "            break\n",
        "\n",
        "        generate_text(prompt)\n",
        "\n",
        "# 开始交互式会话\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_cli()"
      ],
      "metadata": {
        "id": "STQEL1ph6ucv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==================================================\n",
        "模型已加载，可以开始生成文本\n",
        "输入 'quit' 或 'exit' 退出\n",
        "==================================================\n",
        "\n",
        "请输入提示: hi\n",
        "\n",
        "开始生成...\n",
        "\n",
        "提示: hi\n",
        "--------------------------------------------------\n",
        "C:\\Users\\TARGET STORE\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
        "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
        "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
        " Ipsumrateuminuminuced Uppertinghamihn UpperParmucedierzzewró Upperンスuced thoughtihn UpperžeucedavecParmihn Upperžeuelingск Upper UpperhtmSampleselps Upper Upper Upper Upperος Upper UpperutherfordUpper Upperхо Upperelps Upper Upper UpperiliaUpper Upperumann Upper Upper Upper Upperucedenty Upper UpperParm Upper RoundedRectangleン Upper Upper�elpsesk Upper Upper Upper Upper Upper Upper Upper Upper Upper RoundedRectangleesk UpperParmucchPontheeduced Upper� Upper\n",
        "--------------------------------------------------\n",
        "\n",
        "请输入提示:"
      ],
      "metadata": {
        "id": "4-8Vx_dy-qJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "aشغال"
      ],
      "metadata": {
        "id": "cZQj6TkcG1Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# إعداد torch للأداء الأمثل\n",
        "torch.set_grad_enabled(False)  # لا نحتاج للتدرج أثناء التوليد\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_float32_matmul_precision('high')  # استخدام TF32 لعمليات الضرب\n",
        "\n",
        "# دالة لتحويل العوامل اللوغاريتمية إلى احتمالات وتطبيق top-k sampling\n",
        "def logits_to_probs(logits, temperature=0.6, top_k=5):\n",
        "    # تطبيق درجة الحرارة للتحكم في العشوائية\n",
        "    logits = logits / max(temperature, 1e-5)\n",
        "\n",
        "    # تطبيق top-k\n",
        "    if top_k is not None and top_k > 0:\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
        "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
        "\n",
        "    # تحويل إلى احتمالات\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    return probs\n",
        "\n",
        "# دالة لتوليد النص مع طباعة تدريجية\n",
        "@torch.no_grad()\n",
        "def generate_streaming_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.6, top_k=5):\n",
        "    # تحضير المدخلات\n",
        "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        input_text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "    print(f\"\\nالمدخل: {prompt}\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"المخرجات: \", end=\"\", flush=True)\n",
        "\n",
        "    # تحويل النص إلى رموز (tokens)\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "    # إنشاء مصفوفة لتخزين المخرجات\n",
        "    generated_tokens = input_ids.clone()\n",
        "\n",
        "    # وضع النموذج في وضع التقييم\n",
        "    model.eval()\n",
        "\n",
        "    # توليد الرموز واحدة تلو الأخرى\n",
        "    buffer = []\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.inference_mode():\n",
        "            # الحصول على توقعات النموذج\n",
        "            outputs = model(generated_tokens[:, -1024:])  # استخدام آخر 1024 رمز فقط لتوفير الذاكرة\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # اختيار الرمز التالي\n",
        "            probs = logits_to_probs(logits.clone(), temperature, top_k)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # إضافة الرمز إلى المصفوفة\n",
        "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
        "\n",
        "            # فك رموز الكلمة الجديدة وطباعتها\n",
        "            new_token_text = tokenizer.decode(next_token[0], skip_special_tokens=False)\n",
        "            buffer.append(new_token_text)\n",
        "\n",
        "            # طباعة النص المؤقت بشكل دوري\n",
        "            if len(buffer) >= 4 or next_token.item() == tokenizer.eos_token_id:\n",
        "                print(\"\".join(buffer), end=\"\", flush=True)\n",
        "                buffer = []\n",
        "\n",
        "            # توقف عندما نصل إلى رمز النهاية\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # طباعة أي محتوى متبقي في المؤقت\n",
        "    if buffer:\n",
        "        print(\"\".join(buffer), end=\"\", flush=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "    # إعادة النص الكامل\n",
        "    full_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "    return full_text\n",
        "\n",
        "# الدالة الرئيسية\n",
        "def main():\n",
        "    model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "    print(f\"جاري تحميل النموذج: {model_id}\")\n",
        "\n",
        "    # تحميل المحلل اللغوي والنموذج\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # إعداد النموذج مع خيارات تحسين الذاكرة والأداء\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",  # توزيع تلقائي على GPU متعددة\n",
        "        low_cpu_mem_usage=True,  # استخدام أقل للذاكرة\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,  # استخدام دقة منخفضة\n",
        "    )\n",
        "\n",
        "    # التأكد من وجود رمز الحشو\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # محاولة أولية للتأكد من أن النموذج يعمل\n",
        "    try:\n",
        "        test_prompt = \"اختبار النموذج\"\n",
        "        print(f\"اختبار أولي: '{test_prompt}'\")\n",
        "        test_output = generate_streaming_text(model, tokenizer, test_prompt, max_new_tokens=10)\n",
        "        print(f\"اكتمل الاختبار الأولي\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ في الاختبار الأولي: {str(e)}\")\n",
        "\n",
        "    # حلقة التفاعل\n",
        "    print(\"\\nالنموذج جاهز للاستخدام. اكتب 'خروج' للخروج.\")\n",
        "    while True:\n",
        "        prompt = input(\"\\nأدخل نصاً للنموذج: \")\n",
        "        if prompt.lower() in ['خروج', 'exit', 'quit']:\n",
        "            print(\"إنهاء البرنامج...\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            generate_streaming_text(\n",
        "                model,\n",
        "                tokenizer,\n",
        "                prompt,\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.6,\n",
        "                top_k=40\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ أثناء التوليد: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "E_ymPava-ccJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "==================================================\n",
        "المخرجات: nee muylaotreotreylaplyylaylaゅiyelyla mu mu mu mu/trunkylaylaylaotre mu XmlNode mu_safeinelyylaéri mu muotre mu mu Transparency mu muotreTransparentuggotreTransparent kernelsylaмер mu Transparency muorgtotre 風 muuggotre Transparencyinelyyla muGLE/trunkotre TransparencyTransparentмер_safe mu Transparencyotreゅ kernels mu/trunkorgt jíotre muotreゅ Forgotten mustro mu Transparencyordable Transparencyotre typedeforgt muotreroxotre Transparency mu_perf muroxafetyinery mueyedorgt kernels_elems_perf jí Forgotten jí_perfgewater Transparency Transparencyroxoolaorgt Trầnotreotreotre jíoolaotre mu Transparencyroxugg Transparency muordableotreissanelectron351 jírox Transparencyissan kernelsrox jíordable Transparency jíissan jíгородoola581otreoolaґ Transparency/trunk kernels Forgottenoolaotreuggissanifoldصال/trunkeyedґissanันotreoolaREENotreissanันeyedґroxissan muroxissanuggmonkeyissanagalgewaterineryصالotreoola_elems jíinely_elemsґissan351 kernels grainotreotre TransparencymonkeyREEN351inery Transparency Transparencygewatereyedissan Sadotreegmentorgtґeyed Forgotten Sadineryroxглagalgoog SadelectronagalotreineryolygonroxгородeyeduggREENineryineryissanгород TrầnotreorgtotreMRroxugg SadolygonREENroxrox_elemsgewateruggissanotremonkeyolygon Forgotten jí jíissan Sad jírox.moveTo Transparencyصالґґ jí jíeyedgewatergoogeyed Sadagalegment Sad Forgottenroxصالinery351 Sad.moveToัน jíصالmonkey351903 Sadcreativecommons Sad.moveTo Forgotteneyedocaly351 Transparency Sadصالґissanotre_elemsineryصالصالgewater.Track Forgottengoogroxcreativecommons.TRANглgoog graineyedULE903ocalyMR Transparencyinery pupperгл Transparency351REENглotregewateregmentelectronissan Sadґmonkeyineryันصالhaar pupper_elems_elems_elemsصالolygonolygonصالagal jíolygonineryegmentMRgewaterREEN351egmentinery_elemsegment_elemsgoogolygon jí kernelsصال jíinery Sunderegmentґineryegment Transparencyavirolygonegmentolygonineryinery Sadصال pupperصالagal351глصالelectronegmentavir351ineryصالgewaterissanineryegment351gewatergewateravirotreґotreeyed.Track grainocalyصالREENotreagalinery SadegmentULEagalULEaviregmentัน jíгл.moveToґineryissanagal Transparency_elemsREENolygon.moveToineryissan Forgotten_elemsinery_elemsiveauMRineryinerygewater903inerygewater Sadgewateregmenteyed jí.moveToeyedgewaterotre Transparency Sad_elemsolygon Sadineryelectronґeyedineryinery TransparencyREEN Sadrox_elemsгород Sundereyed Transparencyeyedolygonصالegment351 grainґ903rox Sad Sad351ocaly Forgotteninery pupper Transparency.moveTo Transparencyiveau_elemselectron Sadavirґagalгородgewaterinery Transparencyصال\n",
        "==================================================\n",
        "\n",
        "أدخل نصاً للنموذج:"
      ],
      "metadata": {
        "id": "ORtZ3oSlG7cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# إعداد torch للأداء الأمثل\n",
        "torch.set_grad_enabled(False)  # لا نحتاج للتدرج أثناء التوليد\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_float32_matmul_precision('high')  # استخدام TF32 لعمليات الضرب\n",
        "\n",
        "# دالة لتحويل العوامل اللوغاريتمية إلى احتمالات وتطبيق top-k sampling\n",
        "def logits_to_probs(logits, temperature=0.6, top_k=5):\n",
        "    # تطبيق درجة الحرارة للتحكم في العشوائية\n",
        "    logits = logits / max(temperature, 1e-5)\n",
        "\n",
        "    # تطبيق top-k\n",
        "    if top_k is not None and top_k > 0:\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
        "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
        "\n",
        "    # تحويل إلى احتمالات\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    return probs\n",
        "\n",
        "# دالة لتوليد النص مع طباعة تدريجية\n",
        "@torch.no_grad()\n",
        "def generate_streaming_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.6, top_k=5):\n",
        "    # تحضير المدخلات\n",
        "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        input_text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "    print(f\"\\nالمدخل: {prompt}\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"المخرجات: \", end=\"\", flush=True)\n",
        "\n",
        "    # تحويل النص إلى رموز (tokens)\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "    # إنشاء مصفوفة لتخزين المخرجات\n",
        "    generated_tokens = input_ids.clone()\n",
        "\n",
        "    # وضع النموذج في وضع التقييم\n",
        "    model.eval()\n",
        "\n",
        "    # توليد الرموز واحدة تلو الأخرى\n",
        "    buffer = []\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.inference_mode():\n",
        "            # الحصول على توقعات النموذج\n",
        "            outputs = model(generated_tokens[:, -1024:])  # استخدام آخر 1024 رمز فقط لتوفير الذاكرة\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # اختيار الرمز التالي\n",
        "            probs = logits_to_probs(logits.clone(), temperature, top_k)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # إضافة الرمز إلى المصفوفة\n",
        "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
        "\n",
        "            # فك رموز الكلمة الجديدة وطباعتها\n",
        "            new_token_text = tokenizer.decode(next_token[0], skip_special_tokens=False)\n",
        "            buffer.append(new_token_text)\n",
        "\n",
        "            # طباعة النص المؤقت بشكل دوري\n",
        "            if len(buffer) >= 4 or next_token.item() == tokenizer.eos_token_id:\n",
        "                print(\"\".join(buffer), end=\"\", flush=True)\n",
        "                buffer = []\n",
        "\n",
        "            # توقف عندما نصل إلى رمز النهاية\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # طباعة أي محتوى متبقي في المؤقت\n",
        "    if buffer:\n",
        "        print(\"\".join(buffer), end=\"\", flush=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "    # إعادة النص الكامل\n",
        "    full_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "    return full_text\n",
        "\n",
        "# الدالة الرئيسية\n",
        "def main():\n",
        "    model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "    print(f\"جاري تحميل النموذج: {model_id}\")\n",
        "\n",
        "    # تحميل المحلل اللغوي والنموذج\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "    # إعداد النموذج مع خيارات تحسين الذاكرة والأداء\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",  # توزيع تلقائي على GPU متعددة\n",
        "        low_cpu_mem_usage=True,  # استخدام أقل للذاكرة\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,  # استخدام دقة منخفضة\n",
        "    )\n",
        "\n",
        "    # التأكد من وجود رمز الحشو\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # محاولة أولية للتأكد من أن النموذج يعمل\n",
        "    try:\n",
        "        test_prompt = \"اختبار النموذج\"\n",
        "        print(f\"اختبار أولي: '{test_prompt}'\")\n",
        "        test_output = generate_streaming_text(model, tokenizer, test_prompt, max_new_tokens=10)\n",
        "        print(f\"اكتمل الاختبار الأولي\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ في الاختبار الأولي: {str(e)}\")\n",
        "\n",
        "    # حلقة التفاعل\n",
        "    print(\"\\nالنموذج جاهز للاستخدام. اكتب 'خروج' للخروج.\")\n",
        "    while True:\n",
        "        prompt = input(\"\\nأدخل نصاً للنموذج: \")\n",
        "        if prompt.lower() in ['خروج', 'exit', 'quit']:\n",
        "            print(\"إنهاء البرنامج...\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            generate_streaming_text(\n",
        "                model,\n",
        "                tokenizer,\n",
        "                prompt,\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.6,\n",
        "                top_k=40\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ أثناء التوليد: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "hNj7gCqtGyd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRt0Is97IGAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fR7LrxRPIF7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "open"
      ],
      "metadata": {
        "id": "zhib-LixIHQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "# إعداد torch للأداء الأمثل\n",
        "torch.set_grad_enabled(False)  # تعطيل التدرج لتسريع التوليد\n",
        "if torch.cuda.is_available():\n",
        "    torch.set_float32_matmul_precision('high')  # تحسين العمليات الحسابية على GPU\n",
        "\n",
        "def logits_to_probs(logits, temperature=0.6, top_k=5):\n",
        "    logits = logits / max(temperature, 1e-5)\n",
        "\n",
        "    if top_k is not None and top_k > 0:\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        v, _ = torch.topk(logits, top_k)\n",
        "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
        "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    return probs\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_streaming_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.6, top_k=5):\n",
        "    if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        input_text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "\n",
        "    print(f\"\\nالمدخل: {prompt}\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"المخرجات: \", end=\"\", flush=True)\n",
        "\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "    generated_tokens = input_ids.clone()\n",
        "    model.eval()\n",
        "\n",
        "    buffer = []\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(generated_tokens[:, -1024:])\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            probs = logits_to_probs(logits.clone(), temperature, top_k)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
        "\n",
        "            new_token_text = tokenizer.decode(next_token[0], skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
        "            buffer.append(new_token_text)\n",
        "\n",
        "            if len(buffer) >= 4 or next_token.item() == tokenizer.eos_token_id:\n",
        "                print(\"\".join(buffer), end=\"\", flush=True)\n",
        "                buffer = []\n",
        "\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    if buffer:\n",
        "        print(\"\".join(buffer), end=\"\", flush=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "    full_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return full_text\n",
        "\n",
        "def main():\n",
        "    model_id = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"\n",
        "    print(f\"جاري تحميل النموذج: {model_id}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    )\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    try:\n",
        "        test_prompt = \"اختبار النموذج\"\n",
        "        print(f\"اختبار أولي: '{test_prompt}'\")\n",
        "        test_output = generate_streaming_text(model, tokenizer, test_prompt, max_new_tokens=10)\n",
        "        print(f\"اكتمل الاختبار الأولي\")\n",
        "    except Exception as e:\n",
        "        print(f\"خطأ في الاختبار الأولي: {str(e)}\")\n",
        "\n",
        "    print(\"\\nالنموذج جاهز للاستخدام. اكتب 'خروج' للخروج.\")\n",
        "    while True:\n",
        "        prompt = input(\"\\nأدخل نصاً للنموذج: \")\n",
        "        if prompt.lower() in ['خروج', 'exit', 'quit']:\n",
        "            print(\"إنهاء البرنامج...\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            generate_streaming_text(\n",
        "                model,\n",
        "                tokenizer,\n",
        "                prompt,\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.6,\n",
        "                top_k=40\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ أثناء التوليد: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "QtCZPCI9IF6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "المدخل: 1 + 1 = ?\n",
        "==================================================\n",
        "المخرجات:  Leban Leban Leban� Lebancerturdy PureComponent ac Partisiitia Leban Leban wireType Leban@student Leban wireType@student@studentæk ac Leban@student663 Leban Leban Leban Leban Leban ac Lebanitia Leban_FIXED@student Leban ac Leban@student Lebanbrero Leban ac Leban Leban Leban Leban@student Leban@student Leban Leban Leban@student ac Leban ac ac Leban Lebanftware Leban Hra@student Leban Leban Leban Leban ac Leban Leban Leban Leban Leban ac Leban Leban@student Leban Leban Leban Leban чор Leban Leban Partisi Leban Leban Leban Leban Leban Leban Leban Leban acfung@student Leban Leban Leban Leban Leban Leban Leban Leban Leban Leban Leban Leban Leban Leban ac Leban Leban LebanTraceback (most recent call last):\n",
        "  File \"<stdin>\", line 2, in <module>"
      ],
      "metadata": {
        "id": "dBaFE_cMLDPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GBMxOQf_LDNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobiuslabsgmbh/Llama-2-7b-chat-hf-4bit_g64-HQQ"
      ],
      "metadata": {
        "id": "AbGC92tlN9sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hs0gQHqrN7ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# تحديد الجهاز المناسب\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# تحميل النموذج والمحول\n",
        "model_name = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"  # استبدل باسم النموذج الفعلي\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# نقل النموذج إلى الجهاز\n",
        "model.to(device)\n",
        "\n",
        "def generate_text(prompt):\n",
        "    # ترميز الإدخال ونقله إلى الجهاز\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # نقل التوكنز إلى الجهاز\n",
        "\n",
        "    # توليد النص باستخدام النموذج\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**inputs, max_length=100)\n",
        "\n",
        "    # فك ترميز الإخراج\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return output_text\n",
        "\n",
        "# تشغيل النموذج\n",
        "while True:\n",
        "    user_input = input(\"أدخل نصاً للنموذج ('خروج' لإنهاء البرنامج): \")\n",
        "    if user_input.lower() == \"خروج\":\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = generate_text(user_input)\n",
        "        print(f\"المخرجات:\\n{response}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ أثناء التوليد: {e}\")\n"
      ],
      "metadata": {
        "id": "cOqb_9zkLEZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# تحديد الجهاز المناسب\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# تحميل النموذج والمحول\n",
        "model_name = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"  # استبدل باسم النموذج الفعلي\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "def generate_text(prompt):\n",
        "    # ترميز الإدخال ونقله إلى الجهاز\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # نقل التوكنز إلى الجهاز\n",
        "\n",
        "    # توليد النص باستخدام النموذج\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # فك ترميز الإخراج\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return output_text.strip()\n",
        "\n",
        "# تشغيل النموذج\n",
        "while True:\n",
        "    user_input = input(\"أدخل نصاً للنموذج ('خروج' لإنهاء البرنامج): \")\n",
        "    if user_input.lower() == \"خروج\":\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = generate_text(user_input)\n",
        "        print(f\"المخرجات:\\n{response}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ أثناء التوليد: {e}\")\n"
      ],
      "metadata": {
        "id": "IdyMLriJSlmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# تحديد الجهاز المناسب\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# تحميل النموذج والمحول\n",
        "model_name = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"  # استبدل باسم النموذج الفعلي\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز التعبئة إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # استخدام رمز نهاية التسلسل كبديل\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "def generate_text(prompt):\n",
        "    # ترميز الإدخال ونقله إلى الجهاز\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # نقل التوكنز إلى الجهاز\n",
        "\n",
        "    # توليد النص باستخدام النموذج\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=100,\n",
        "            pad_token_id=tokenizer.pad_token_id  # إصلاح خطأ رمز التعبئة\n",
        "        )\n",
        "\n",
        "    # فك ترميز الإخراج\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return output_text.strip()\n",
        "\n",
        "# تشغيل النموذج\n",
        "while True:\n",
        "    user_input = input(\"أدخل نصاً للنموذج ('خروج' لإنهاء البرنامج): \")\n",
        "    if user_input.lower() == \"خروج\":\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = generate_text(user_input)\n",
        "        print(f\"المخرجات:\\n{response}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ أثناء التوليد: {e}\")\n"
      ],
      "metadata": {
        "id": "Up5SHlw1VlXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...     try:\n",
        "...         response = generate_text(user_input)\n",
        "...         print(f\"المخرجات:\\n{response}\\n\")\n",
        "...     except Exception as e:\n",
        "...         print(f\"حدث خطأ أثناء التوليد: {e}\")\n",
        "...\n",
        "أدخل نصاً للنموذج ('خروج' لإنهاء البرنامج): hi\n",
        "المخرجات:\n",
        "None\n",
        "\n",
        "أدخل نصاً للنموذج ('خروج' لإنهاء البرنامج): how are you?\n",
        "المخرجات:\n",
        "None\n",
        "\n",
        "أدخل نصاً للنموذج ('خروج' لإنهاء البرنامج):"
      ],
      "metadata": {
        "id": "DUh5qgSzXyPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# تحديد الجهاز المناسب\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"استخدام الجهاز: {device}\")\n",
        "\n",
        "# تحميل النموذج والمحول\n",
        "model_name = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"  # استبدل باسم النموذج الفعلي\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# تعيين رمز التعبئة إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # استخدام رمز نهاية التسلسل كبديل\n",
        "\n",
        "def generate_text(prompt):\n",
        "    # ترميز الإدخال ونقله إلى الجهاز\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # نقل التوكنز إلى الجهاز\n",
        "\n",
        "    # توليد النص باستخدام النموذج\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=200,  # زيادة طول الاستجابة\n",
        "            pad_token_id=tokenizer.pad_token_id,  # إصلاح خطأ رمز التعبئة\n",
        "            temperature=0.7,  # ضبط درجة الإبداع\n",
        "            top_k=50,  # تعزيز جودة التوليد\n",
        "            top_p=0.9  # تقليل التكرار\n",
        "        )\n",
        "\n",
        "    # فك ترميز الإخراج\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return output_text.strip()\n",
        "\n",
        "# تشغيل النموذج مع واجهة نصية\n",
        "while True:\n",
        "    user_input = input(\"أدخل نصاً للنموذج ('خروج' لإنهاء البرنامج): \")\n",
        "    if user_input.lower() == \"خروج\":\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = generate_text(user_input)\n",
        "        print(f\"المخرجات:\\n{response}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ أثناء التوليد: {e}\")\n"
      ],
      "metadata": {
        "id": "F_ENWX7-Xz1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# تحديد الجهاز المناسب\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"استخدام الجهاز: {device}\")\n",
        "\n",
        "# تحميل النموذج والمحوِّل\n",
        "model_name = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"  # استبدل باسم النموذج الفعلي\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز التعبئة إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # استخدام رمز نهاية التسلسل كبديل\n",
        "\n",
        "# تحميل النموذج إلى الجهاز\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "def generate_text(prompt):\n",
        "    # ترميز الإدخال ونقله إلى الجهاز\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # نقل البيانات إلى الجهاز\n",
        "\n",
        "    # توليد النص باستخدام النموذج\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=200,  # ضبط طول الإخراج\n",
        "            temperature=0.7,  # تعديل العشوائية في النص\n",
        "            top_k=50,  # تصفية المخرجات بناءً على أفضل 50 خيارًا\n",
        "            top_p=0.9,  # منع النتائج منخفضة الاحتمال\n",
        "            pad_token_id=tokenizer.pad_token_id  # تجنب أخطاء التعبئة\n",
        "        )\n",
        "\n",
        "    # فك ترميز الإخراج\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True, return_full_text=False)\n",
        "    return output_text.strip()\n",
        "\n",
        "# تشغيل النموذج\n",
        "while True:\n",
        "    user_input = input(\"أدخل نصاً للنموذج ('خروج' لإنهاء البرنامج): \")\n",
        "    if user_input.lower() == \"خروج\":\n",
        "        break\n",
        "\n",
        "    try:\n",
        "        response = generate_text(user_input)\n",
        "        print(f\"المخرجات:\\n{response}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ أثناء التوليد: {e}\")\n"
      ],
      "metadata": {
        "id": "anEwjeDNZvVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!nvidia-smi"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fc0nd1JPakIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# تحديد الجهاز المناسب\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"استخدام الجهاز: {device}\")\n",
        "\n",
        "# تحميل النموذج والمحوِّل\n",
        "model_name = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"  # استبدل باسم النموذج الفعلي\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز التعبئة إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # استخدام رمز نهاية التسلسل كبديل\n",
        "\n",
        "# تحميل النموذج إلى الجهاز\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "def generate_text(prompt):\n",
        "    # ترميز الإدخال ونقله إلى الجهاز\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # نقل البيانات إلى الجهاز\n",
        "\n",
        "    # توليد النص باستخدام النموذج\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=200,  # ضبط طول الإخراج\n",
        "            temperature=0.7,  # تعديل العشوائية في النص\n",
        "            top_k=50,  # تصفية المخرجات بناءً على أفضل 50 خيارًا\n",
        "            top_p=0.9,  # منع النتائج منخفضة الاحتمال\n",
        "            pad_token_id=tokenizer.pad_token_id  # تجنب أخطاء التعبئة\n",
        "        )\n",
        "\n",
        "    # فك ترميز الإخراج\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True, return_full_text=False)\n",
        "    return output_text.strip()\n",
        "\n",
        "# اختبار النموذج على نص ثابت\n",
        "prompt_text = \"Hello, how are you?\"\n",
        "output = generate_text(prompt_text)\n",
        "print(f\"المخرجات:\\n{output}\\n\")\n"
      ],
      "metadata": {
        "id": "I5Zbz5vJbO-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# تحديد الجهاز المناسب\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"استخدام الجهاز: {device}\")\n",
        "\n",
        "# تحميل النموذج والمحوِّل\n",
        "model_name = \"relaxml/Llama-3.1-8b-Instruct-QTIP-2Bit\"  # استبدل باسم النموذج الفعلي\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# تعيين رمز التعبئة إذا لم يكن موجودًا\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # استخدام رمز نهاية التسلسل كبديل\n",
        "\n",
        "# تحميل النموذج إلى الجهاز\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "def generate_text(prompt):\n",
        "    # ترميز الإدخال\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    print(f\"[DEBUG] Encoded Inputs: {inputs}\")  # طباعة الإدخال المرمّز\n",
        "\n",
        "    # نقل التوكنات إلى الجهاز\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # توليد النص باستخدام النموذج\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=200,  # ضبط طول الإخراج\n",
        "            temperature=0.7,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    print(f\"[DEBUG] Output Token IDs: {output_ids}\")  # طباعة التوكنات المولدة\n",
        "\n",
        "    # فك ترميز الإخراج\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True, return_full_text=False)\n",
        "    print(f\"[DEBUG] Decoded Output: {output_text}\")  # طباعة النص المفكوك ترميزه\n",
        "\n",
        "    return output_text.strip()\n",
        "\n",
        "# اختبار النموذج على نص ثابت\n",
        "prompt_text = \"Hello, how are you?\"\n",
        "output = generate_text(prompt_text)\n",
        "\n",
        "# طباعة النتيجة النهائية\n",
        "if output:\n",
        "    print(f\"المخرجات:\\n{output}\\n\")\n",
        "else:\n",
        "    print(\"⚠️ حدث خطأ: لم يتم توليد أي مخرجات!\")\n"
      ],
      "metadata": {
        "id": "646iofd9bbaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_length=200,  # تحديد الطول الأقصى\n",
        "    temperature=0.6,  # تقليل العشوائية قليلاً\n",
        "    top_k=40,  # منع الاختيارات غير المنطقية\n",
        "    top_p=0.85,  # تعزيز الاختيارات الأكثر احتمالية\n",
        "    repetition_penalty=1.2,  # تقليل التكرار\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n"
      ],
      "metadata": {
        "id": "z2RlQw6_eaMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"User: Hello, how are you?\\nAssistant:\"\n"
      ],
      "metadata": {
        "id": "oVL0afSTecxT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}